{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210229d8",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "Import the required Python libraries for HTTP requests, data manipulation, HTML parsing, and time control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d57a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "#import time from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b95442",
   "metadata": {},
   "source": [
    "## MyTheresaScraper Class Definition\n",
    "This class is designed to encapsulate the methods needed for scraping data from the MyTheresa website specifically for men's fashion. It provides functionalities to fetch and parse webpage content, extract categories, subcategories, and product details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f636e",
   "metadata": {},
   "source": [
    "### Constructor\n",
    "Initializes a new instance of the `MyTheresaScraper` class with a base URL. The base URL points to the men's section of the MyTheresa website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec057522",
   "metadata": {},
   "source": [
    "### Get Request Page Method\n",
    "Attempts to fetch content from a given URL derived from the base URL and a subpath. If the request fails, it retries up to a maximum number of retries (`max_retries`) and pauses (`retry_delay`) between retries. If all attempts fail, it returns `None`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1315ba4",
   "metadata": {},
   "source": [
    "### Get Categories Method\n",
    "Fetches the homepage to extract main categories and subcategories. It constructs a DataFrame to keep the data organized. Categories are assumed to be in specific divisions of the webpage, hence the slice `[2:7]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3e8d5",
   "metadata": {},
   "source": [
    "### Get Subcategories Method\n",
    "For a given category page, this method extracts all subcategories and their corresponding URLs, updating the main categories DataFrame with these details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa30f21",
   "metadata": {},
   "source": [
    "### Get Detail Method\n",
    "Extracts detailed information of all products listed on a category-specific page. It collects data such as designer name, item name, and price. This method returns a list of dictionaries, each containing a product's complete details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bd693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c46be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTheresaScraper:\n",
    "    def __init__(self, base_url='https://www.mytheresa.com/ch/en/men'):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def get_request_page(self, sub_url, max_retries=10, retry_delay=5):\n",
    "        retries = 0\n",
    "        url = f'{self.base_url}/{sub_url}'\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "            except requests.RequestException as error:\n",
    "                if \"Max retries exceeded\" in str(error):\n",
    "                    retries += 1\n",
    "                    time.sleep(retry_delay)\n",
    "                    print('Retry...')\n",
    "                else:\n",
    "                    print(f'Error fetching page: {error}')\n",
    "                    return None\n",
    "        print(f'Max retries exceeded. Skipping {url}')\n",
    "        return None\n",
    "\n",
    "    def get_categories(self):\n",
    "        categories_df = pd.DataFrame(columns=['Category', 'Subcategory', 'Link'])\n",
    "        homepage = self.get_request_page('')\n",
    "        if homepage:\n",
    "            categories_pages = homepage.find_all('div', class_='nav__item')\n",
    "            for category_page in categories_pages[2:7]:\n",
    "                category = category_page.find('a', class_='nav__item__text__link', href=True).text.strip()\n",
    "                categories_df = self.get_subcategories(categories_df, category_page, category)\n",
    "        return categories_df\n",
    "\n",
    "    def get_subcategories(self, categories_df, sub_page, category):\n",
    "        subpage = sub_page.find('ul', class_='flyout__col__content__list')\n",
    "        if subpage:\n",
    "            subcategories_pages = subpage.find_all('a', class_='flyout__col__content__list__item__link', href=True)\n",
    "            new_rows = [{\n",
    "                'Category': category,\n",
    "                'Subcategory': subcategory_page.text.strip(),\n",
    "                'Link': subcategory_page['href'].split(\"/men/\", 1)[-1]\n",
    "            } for subcategory_page in subcategories_pages]\n",
    "            categories_df = pd.concat([categories_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        return categories_df\n",
    "\n",
    "    def get_detail(self, category_row, detail_page):\n",
    "        items = detail_page.find_all('div', class_=\"item\")\n",
    "        detail_data = []\n",
    "        for item in items:\n",
    "            designer_name = item.find('div', class_=\"item__info__header__designer\").text.strip()\n",
    "            item_name = item.find('div', class_=\"item__info__name\").find('a').text.strip()\n",
    "            price = item.find('span', class_=\"pricing__prices__price\").text.strip()\n",
    "            detail_data.append({\n",
    "                'Category': category_row['Category'],\n",
    "                'Subcategory': category_row['Subcategory'],\n",
    "                'Designer': designer_name,\n",
    "                'Name': item_name,\n",
    "                'Price': price\n",
    "            })\n",
    "        return detail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355a472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9ffb185",
   "metadata": {},
   "source": [
    "## Main Execution Block\n",
    "This block checks if the script is run as the main program and performs the scraping process through multiple pages, collecting all necessary product details and eventually saving them to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e28b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    scraper = MyTheresaScraper()\n",
    "    categories = scraper.get_categories()\n",
    "    all_data = []\n",
    "    \n",
    "    for index, row in categories.iterrows():\n",
    "        page = scraper.get_request_page(f'{row[\"Link\"]}?sortBy=new_item&page=1')\n",
    "        if page:\n",
    "            all_data.extend(scraper.get_detail(row, page))\n",
    "            # Attempt to get the maximum page number\n",
    "            max_page_element = page.find('a', {'data-label': 'lastPage'})\n",
    "            if max_page_element:\n",
    "                max_page = int(max_page_element['data-index'])\n",
    "                for x in range(2, max_page + 1):\n",
    "                    page = scraper.get_request_page(f'{row[\"Link\"]}?sortBy=new_item&page={x}')\n",
    "                    if page:\n",
    "                        all_data.extend(scraper.get_detail(row, page))\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df = pd.DataFrame(all_data)\n",
    "    try:\n",
    "        df.to_csv('mytheresa_men_data.csv', index=False)\n",
    "        print(\"CSV file saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd6a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf73f347",
   "metadata": {},
   "source": [
    "## Load Data and Preliminary Analysis\n",
    "\n",
    "Loading the data using Pandas and examining the first few rows along with a summary to identify potential impurities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'mytheresa_men_data.csv'  \n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of the dataframe:\")\n",
    "print(data.head())\n",
    "\n",
    "# General information about the dataframe\n",
    "print(\"\\\\nGeneral information and data types:\")\n",
    "print(data.info())\n",
    "\n",
    "# Description of data in dataframe\n",
    "print(\"\\\\nSummary statistics and unique value counts:\")\n",
    "print(data.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46942e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42bd3c06",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "\n",
    "From the loaded data, here are a few observations:\n",
    "\n",
    "- **Total Entries**: There are 11,371 rows across 5 columns.\n",
    "- **Columns**: The columns included are `Category`, `Subcategory`, `Designer`, `Name`, and `Price`.\n",
    "- **Missing Values**: There appear to be no missing values as each column has entries equal to the row count.\n",
    "- **Data Types**: All columns are currently of type object (string).\n",
    "- **Unique Values**:\n",
    "  - `Category`: 4 unique values.\n",
    "  - `Subcategory`: 46 unique values.\n",
    "  - `Designer`: 140 unique designers.\n",
    "  - `Name`: 7,005 unique product names.\n",
    "  - `Price`: Prices are formatted as strings with the CHF symbol, and there are 1,658 unique price points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc3db6",
   "metadata": {},
   "source": [
    "### Identified Impurities\n",
    "\n",
    "1. **Price Formatting**:\n",
    "   - Prices are prefixed with \"CHF\" and may contain commas in thousands. This needs to be cleaned for numerical operations.\n",
    "2. **Text Formatting**:\n",
    "   - Textual data in `Category`, `Subcategory`, `Designer`, and `Name` should be standardized (e.g., consistent capitalization).\n",
    "3. **Duplicate Entries**:\n",
    "   - With products like \"Cotton polo shirt\" appearing 88 times, it's possible some entries are duplicates that should be investigated and removed.\n",
    "4. **Incorrect Data Types**:\n",
    "   - All columns are read as objects; `Price` should be converted to a numerical format for any financial analysis.\n",
    "5. **Potential Outliers or Incorrect Entries**:\n",
    "   - There could be misentries especially in product names or unusually high or low prices which should be investigated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8067e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def remove_currency_symbols(self):\n",
    "        \"\"\"Remove currency symbols from the 'Price' column and convert to float.\"\"\"\n",
    "        self.df['Price'] = self.df['Price'].replace('[\\$,CHF]', '', regex=True).replace(',', '', regex=True).astype(float)\n",
    "\n",
    "    def standardize_text(self):\n",
    "        \"\"\"Standardize textual data to capitalize (Title case).\"\"\"\n",
    "        text_cols = ['Category', 'Subcategory', 'Designer', 'Name']\n",
    "        for col in text_cols:\n",
    "            self.df[col] = self.df[col].str.title()\n",
    "\n",
    "    #def remove_duplicates(self):\n",
    "        #\"\"\"Remove duplicate rows from the DataFrame.\"\"\"\n",
    "        #self.df.drop_duplicates(inplace=True)\n",
    "        \n",
    "    def check_and_remove_duplicates(self):\n",
    "        \"\"\"Check for and remove duplicate rows from the DataFrame if any.\"\"\"\n",
    "        if self.df.duplicated().sum() > 0:\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            print(\"Duplicates were found and removed.\")\n",
    "        else:\n",
    "            print(\"No duplicates found.\")\n",
    "\n",
    "\n",
    "    def convert_data_types(self):\n",
    "        \"\"\"Convert data types after cleaning.\"\"\"\n",
    "        self.df['Price'] = pd.to_numeric(self.df['Price'], errors='coerce')\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Run all cleaning methods.\"\"\"\n",
    "        self.standardize_text()\n",
    "        self.remove_currency_symbols()\n",
    "        #self.remove_duplicates()\n",
    "        self.check_and_remove_duplicates()\n",
    "        self.convert_data_types()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc45784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and clean the data\n",
    "cleaner = DataCleaner(data)\n",
    "clean_data = cleaner.clean_data()\n",
    "\n",
    "# Show cleaned data info and head\n",
    "clean_data_info = clean_data.info()\n",
    "clean_data_head = clean_data.head()\n",
    "\n",
    "clean_data_head, clean_data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff138e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79257f57",
   "metadata": {},
   "source": [
    "## Cleaned Data Overview\n",
    "\n",
    "The dataset has been successfully cleaned with the following adjustments:\n",
    "\n",
    "### Standardized Text\n",
    "- **Text Fields Affected**: `Category`, `Subcategory`, `Designer`, `Name`\n",
    "- **Action Taken**: All text fields have been converted to title case to ensure consistency across entries.\n",
    "\n",
    "### Price Formatting\n",
    "- **Original Format**: Prices were formatted as strings with \"CHF\" symbols and commas (e.g., \"CHF 2,380\").\n",
    "- **Actions Taken**:\n",
    "  - Removed the \"CHF\" currency symbol and any commas.\n",
    "  - Converted the `Price` field from string to float for numerical operations.\n",
    "\n",
    "### Duplicate Rows\n",
    "- **Initial Check**: A check for duplicate entries was conducted to ensure data quality.\n",
    "- **Result of Check**: No duplicates were found in the dataset. Hence, no entries were removed on this basis.\n",
    "- **Entries Before**: 11,371 entries.\n",
    "- **Entries After**: 11,371 entries (unchanged due to no duplicates).\n",
    "\n",
    "### Data Types Corrections\n",
    "- **Price Field**:\n",
    "  - Converted from string to float, correctly aligning the data type with the content for financial analysis.\n",
    "- **Other Fields**:\n",
    "  - The fields `Category`, `Subcategory`, `Designer`, and `Name` remain as objects (appropriate for string data).\n",
    "\n",
    "### DataFrame Overview After Cleaning\n",
    "\n",
    "```plaintext\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 11,371 entries, 0 to 11,370\n",
    "Data columns (total 5 columns):\n",
    " #   Column       Non-Null Count  Dtype  \n",
    "---  ------       --------------  -----  \n",
    " 0   Category     11,371 non-null  object \n",
    " 1   Subcategory  11,371 non-null  object \n",
    " 2   Designer     11,371 non-null  object \n",
    " 3   Name         11,371 non-null  object \n",
    " 4   Price        11,371 non-null  float64\n",
    "dtypes: float64(1), object(4)\n",
    "memory usage: 444.3+ KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba07eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f11e13",
   "metadata": {},
   "source": [
    "Scenario Overview\n",
    "\n",
    "I will:\n",
    "\n",
    "Dirty the Data: Intentionally introduce common data issues into a clean dataset.\n",
    "Clean the Data: Apply various cleaning techniques to restore the dataset to its original quality.\n",
    "\n",
    "Step 1: Dirtying the Data\n",
    "Objective: Simulate common data issues including random noise in numerical data, text case inconsistencies, and duplicate entries.\n",
    "\n",
    "Step 2: Cleaning the Data\n",
    "Objective: Implement cleaning operations to correct the issues introduced in Step 1, ensuring the data is standardized and duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441675ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fef795a",
   "metadata": {},
   "source": [
    "Data Dirtier Class\n",
    "\n",
    "This class introduces three types of impurities: random noise in prices, random text case errors in string fields, and duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d8f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe53925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDirtier:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def add_noise_to_prices(self):\n",
    "        \"\"\"Add random noise to price: +/- 10% of the current price, ensure 'Price' is float.\"\"\"\n",
    "        self.df['Price'] = self.df['Price'].astype(float)  # Ensure Price is float before adding noise\n",
    "        noise = np.random.uniform(-0.1, 0.1, size=len(self.df))\n",
    "        self.df['Price'] += self.df['Price'] * noise\n",
    "\n",
    "    def introduce_text_errors(self):\n",
    "        \"\"\"Introduce random uppercase letters to 'Designer' and 'Name' fields.\"\"\"\n",
    "        self.df['Designer'] = self.df['Designer'].apply(lambda x: ''.join(\n",
    "            np.random.choice([k.upper(), k.lower()]) for k in x))\n",
    "        self.df['Name'] = self.df['Name'].apply(lambda x: ''.join(\n",
    "            np.random.choice([k.upper(), k.lower()]) for k in x))\n",
    "\n",
    "    def add_duplicate_entries(self):\n",
    "        \"\"\"Duplicate random 5% of the entries.\"\"\"\n",
    "        duplicates = self.df.sample(frac=0.05, replace=False)\n",
    "        self.df = pd.concat([self.df, duplicates], ignore_index=True)\n",
    "\n",
    "    def dirty_data(self):\n",
    "        \"\"\"Run all methods to dirty the data.\"\"\"\n",
    "        self.add_noise_to_prices()\n",
    "        self.introduce_text_errors()\n",
    "        self.add_duplicate_entries()\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd95db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'clean_data' is the DataFrame loaded with original clean data\n",
    "#dirtier = DataDirtier(clean_data.copy())  # Use a copy to preserve the original clean data\n",
    "#dirty_data = dirtier.dirty_data()\n",
    "\n",
    "#cleaner = DataCleaner(dirty_data)\n",
    "#re_cleaned_data = cleaner.clean_data()\n",
    "\n",
    "# Display and save using DataShowcase\n",
    "#data_showcase = DataShowcase(dirty_data, re_cleaned_data)\n",
    "#data_showcase.display_data()\n",
    "#data_showcase.save_data('dirty_data.csv', 'clean_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8399208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataShowcase:\n",
    "    def __init__(self, dirty_df, clean_df):\n",
    "        \"\"\"\n",
    "        Initializes the DataShowcase object with dirty and clean dataframes.\n",
    "        \n",
    "        :param dirty_df: DataFrame before cleaning\n",
    "        :param clean_df: DataFrame after cleaning\n",
    "        \"\"\"\n",
    "        self.dirty_df = dirty_df\n",
    "        self.clean_df = clean_df\n",
    "\n",
    "    def display_data(self):\n",
    "        \"\"\"\n",
    "        Displays the first few rows of the dirty and clean dataframes for comparison.\n",
    "        \"\"\"\n",
    "        print(\"Dirty Data (first 5 rows):\")\n",
    "        display(self.dirty_df.head())\n",
    "        print(\"\\nClean Data (first 5 rows):\")\n",
    "        display(self.clean_df.head())\n",
    "\n",
    "    def save_data(self, dirty_file_path, clean_file_path):\n",
    "        \"\"\"\n",
    "        Saves the dirty and clean dataframes to CSV files and provides download links.\n",
    "        \n",
    "        :param dirty_file_path: Path to save the dirty data CSV\n",
    "        :param clean_file_path: Path to save the clean data CSV\n",
    "        \"\"\"\n",
    "        # Save dirty data\n",
    "        self.dirty_df.to_csv(dirty_file_path, index=False)\n",
    "        print(f\"Dirty data saved to {dirty_file_path}\")\n",
    "\n",
    "        # Save clean data\n",
    "        self.clean_df.to_csv(clean_file_path, index=False)\n",
    "        print(f\"Clean data saved to {clean_file_path}\")\n",
    "\n",
    "        # Provide download links (Assuming running in Jupyter or compatible environment)\n",
    "        print(\"\\nDownload Links:\")\n",
    "        print(f\"Download Dirty Data: [here](./{dirty_file_path})\")\n",
    "        print(f\"Download Clean Data: [here](./{clean_file_path})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of DataShowcase class\n",
    "data_showcase = DataShowcase(dirty_data, re_cleaned_data)\n",
    "data_showcase.display_data()\n",
    "data_showcase.save_data('dirty_data.csv', 'clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9743d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7db330b9",
   "metadata": {},
   "source": [
    "Data Cleaner Class\n",
    "\n",
    "This class rectifies the issues: it normalizes text fields, corrects numerical fields, and removes duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7718fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def remove_currency_symbols_and_commas(self):\n",
    "        \"\"\"Remove currency symbols and commas from 'Price' and convert to float.\"\"\"\n",
    "        if self.df['Price'].dtype == 'O':  # Checks if 'Price' is object type (string)\n",
    "            self.df['Price'] = self.df['Price'].str.replace('[\\$,CHF]', '', regex=True).replace(',', '', regex=True)\n",
    "        self.df['Price'] = self.df['Price'].astype(float)\n",
    "\n",
    "    def standardize_text(self):\n",
    "        \"\"\"Standardize textual data to capitalize (Title case).\"\"\"\n",
    "        text_cols = ['Category', 'Subcategory', 'Designer', 'Name']\n",
    "        for col in text_cols:\n",
    "            self.df[col] = self.df[col].str.title()\n",
    "\n",
    "    def check_and_remove_duplicates(self):\n",
    "        \"\"\"Check for and remove duplicate rows from the DataFrame if any.\"\"\"\n",
    "        if self.df.duplicated().sum() > 0:\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            print(\"Duplicates were found and removed.\")\n",
    "        else:\n",
    "            print(\"No duplicates found.\")\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Run all cleaning methods.\"\"\"\n",
    "        self.standardize_text()\n",
    "        self.remove_currency_symbols_and_commas()\n",
    "        self.check_and_remove_duplicates()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b00a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4eda0a",
   "metadata": {},
   "source": [
    "Output and Validation\n",
    "\n",
    "This process ensures that despite the intentional introduction of errors, the data can be restored effectively using the DataCleaner class. The output will verify that text fields are standardized, prices are correctly formatted as numeric values without any symbols or commas, and any inadvertently introduced duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'clean_data' is the DataFrame obtained from previous clean operations\n",
    "# Instantiate the dirtier and apply dirty methods\n",
    "dirtier = DataDirtier(clean_data)\n",
    "dirty_data = dirtier.dirty_data()\n",
    "\n",
    "# Now clean the dirty data\n",
    "cleaner = DataCleaner(dirty_data)\n",
    "re_cleaned_data = cleaner.clean_data()\n",
    "\n",
    "# Display the info of the re-cleaned data\n",
    "print(re_cleaned_data.info())\n",
    "print(re_cleaned_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c346d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
